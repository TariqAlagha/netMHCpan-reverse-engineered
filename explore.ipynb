{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4999b028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>binding_affinity</th>\n",
       "      <th>mhc_allele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAYAAM</td>\n",
       "      <td>0.177415</td>\n",
       "      <td>H-2-Db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAYAAM</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>H-2-Kb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAFEAAL</td>\n",
       "      <td>0.362118</td>\n",
       "      <td>BoLA-3:00101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAFEAAL</td>\n",
       "      <td>0.468035</td>\n",
       "      <td>BoLA-3:00201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAFEAAL</td>\n",
       "      <td>0.522653</td>\n",
       "      <td>HLA-B48:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      peptide  binding_affinity    mhc_allele\n",
       "0  AAAAAAYAAM          0.177415        H-2-Db\n",
       "1  AAAAAAYAAM          0.463100        H-2-Kb\n",
       "2   AAAAFEAAL          0.362118  BoLA-3:00101\n",
       "3   AAAAFEAAL          0.468035  BoLA-3:00201\n",
       "4   AAAAFEAAL          0.522653    HLA-B48:01"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Robust, streaming parse to avoid pandas tokenizer OOM\n",
    "files = ['data/netMHCpan_training_data/BA/c000_ba', 'data/netMHCpan_training_data/BA/c001_ba',\n",
    "         'data/netMHCpan_training_data/BA/c002_ba', 'data/netMHCpan_training_data/BA/c003_ba',\n",
    "         'data/netMHCpan_training_data/BA/c004_ba']\n",
    "\n",
    "peptides, affinities, alleles = [], [], []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            pep, aff_str, allele = parts[0], parts[1], parts[2]\n",
    "            try:\n",
    "                aff = float(aff_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            peptides.append(pep)\n",
    "            affinities.append(aff)\n",
    "            alleles.append(allele)\n",
    "\n",
    "fullFrame = pd.DataFrame({\n",
    "    'peptide': peptides,\n",
    "    'binding_affinity': pd.Series(affinities, dtype='float32'),\n",
    "    'mhc_allele': alleles,\n",
    "})\n",
    "\n",
    "fullFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b0cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BabyNetMHCpan(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Peptide: one-hot encode → small CNN\n",
    "        self.peptide_encoder = nn.Conv1d(20, 64, kernel_size=3)\n",
    "        \n",
    "        # MHC: pseudo-sequence → embedding\n",
    "        self.mhc_encoder = nn.Linear(34 * 20, 64)  # Just flatten one-hot\n",
    "        \n",
    "        # Combine and predict\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output in [0,1] for BA\n",
    "        )\n",
    "    \n",
    "    def forward(self, peptide, mhc_pseudo):\n",
    "        # The magic happens here\n",
    "        p = F.relu(self.peptide_encoder(peptide))\n",
    "        p = F.adaptive_max_pool1d(p, 1).squeeze(-1)\n",
    "\n",
    "        if mhc_pseudo.dim() == 3:\n",
    "            mhc_pseudo = mhc_pseudo.reshape(mhc_pseudo.size(0), -1)\n",
    "        m = F.relu(self.mhc_encoder(mhc_pseudo))\n",
    "\n",
    "        z = torch.cat([p, m], dim=1)\n",
    "        return self.predictor(z)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9b7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC pseudo coverage: 100.0% of BA rows\n"
     ]
    }
   ],
   "source": [
    "# Parse MHC pseudo-sequences and compute coverage vs BA rows\n",
    "from pathlib import Path\n",
    "\n",
    "pseudo_path = Path('data/netMHCpan_training_data/MHC_pseudo.dat')\n",
    "allele_to_pseudo_raw = {}\n",
    "with pseudo_path.open() as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        allele, seq = parts[0], parts[1]\n",
    "        allele_to_pseudo_raw[allele] = seq\n",
    "\n",
    "# Keep only canonical 34-aa pseudo sequences to match model expectation (34 * 20)\n",
    "allele_to_pseudo = {a: s for a, s in allele_to_pseudo_raw.items() if len(s) == 34}\n",
    "\n",
    "coverage = (fullFrame['mhc_allele'].isin(allele_to_pseudo)).mean()\n",
    "print(f\"MHC pseudo coverage: {coverage*100:.1f}% of BA rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4165f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-name coverage: 100.0% | Canonical coverage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Canonicalize allele names to improve pseudo coverage and matching\n",
    "import re\n",
    "\n",
    "def canonicalize_allele(a: str) -> str:\n",
    "    if not isinstance(a, str):\n",
    "        return \"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9]\", \"\", a).lower()\n",
    "\n",
    "# Build canonical map for 34-aa pseudos\n",
    "canon_to_pseudo_all = {canonicalize_allele(a): s for a, s in allele_to_pseudo_raw.items()}\n",
    "canon_to_pseudo_34 = {k: v for k, v in canon_to_pseudo_all.items() if len(v) == 34}\n",
    "\n",
    "# Compare exact vs canonical coverage\n",
    "exact_cov = (fullFrame['mhc_allele'].isin(allele_to_pseudo)).mean()\n",
    "canon_cov = fullFrame['mhc_allele'].map(lambda a: canonicalize_allele(a) in canon_to_pseudo_34).mean()\n",
    "print(f\"Exact-name coverage: {exact_cov*100:.1f}% | Canonical coverage: {canon_cov*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f957a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding utilities (index-based, GPU one-hot later)\n",
    "AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")  # 20 standard AAs in fixed order\n",
    "AA_TO_INDEX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "PAD_INDEX = 20  # reserved padding index (not used by actual amino acids)\n",
    "\n",
    "\n",
    "def seq_to_index_tensor(seq: str):\n",
    "    import torch\n",
    "    idxs = [AA_TO_INDEX.get(aa, 0) for aa in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e67d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Torch Dataset for BA data (index-based, pickle-safe for multiprocessing)\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class BADataset(Dataset):\n",
    "    def __init__(self, df, allele_to_pseudo):\n",
    "        # Filter rows to those we have pseudo sequences for\n",
    "        self.df = df[df['mhc_allele'].isin(allele_to_pseudo)].reset_index(drop=True)\n",
    "        self.targets = self.df['binding_affinity'].astype('float32').to_numpy()\n",
    "        self.peptides = self.df['peptide'].tolist()\n",
    "        self.alleles = self.df['mhc_allele'].tolist()\n",
    "        # Precompute pseudo indices per unique allele as plain Python lists (avoid torch tensors here)\n",
    "        unique_alleles = sorted(set(self.alleles))\n",
    "        self.allele_to_pseudo_idx_list = {\n",
    "            a: [AA_TO_INDEX.get(aa, 0) for aa in allele_to_pseudo[a]] for a in unique_alleles\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import torch\n",
    "        # Convert to tensors inside worker to avoid sharing storages across processes\n",
    "        pep_idx = torch.tensor([AA_TO_INDEX.get(aa, 0) for aa in self.peptides[idx]], dtype=torch.long)\n",
    "        pseudo_idx = torch.tensor(self.allele_to_pseudo_idx_list[self.alleles[idx]], dtype=torch.long)\n",
    "        y = self.targets[idx]\n",
    "        return pep_idx, pseudo_idx, y\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    import torch\n",
    "    pep_list, pseudo_list, y_list = zip(*batch)\n",
    "    pep_padded = pad_sequence(pep_list, batch_first=True, padding_value=PAD_INDEX)  # (B, Lmax)\n",
    "    pseudo_idx = torch.stack(pseudo_list, dim=0)  # (B, 34)\n",
    "    y = torch.tensor(y_list, dtype=torch.float32).unsqueeze(-1)  # (B,1)\n",
    "    return pep_padded, pseudo_idx, y\n",
    "\n",
    "\n",
    "dataset = BADataset(fullFrame, allele_to_pseudo)\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c87674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187283, 20810)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/val split and DataLoaders\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds = Subset(dataset, val_idx)\n",
    "\n",
    "batch_size = 512  # adjust higher if GPU memory allows\n",
    "\n",
    "# Stable, single-process loaders to avoid shared-memory mmap errors in containers\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84ea2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda')\n",
    "else:\n",
    "    print('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ab7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33538/1370713253.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')\n",
      "/tmp/ipykernel_33538/1370713253.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
      "/tmp/ipykernel_33538/1370713253.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | val MSE: 0.0703\n",
      "Epoch 2 | val MSE: 0.0639\n",
      "Epoch 3 | val MSE: 0.0575\n",
      "Epoch 4 | val MSE: 0.0527\n",
      "Epoch 5 | val MSE: 0.0488\n",
      "Epoch 6 | val MSE: 0.0465\n",
      "Epoch 7 | val MSE: 0.0453\n",
      "Epoch 8 | val MSE: 0.0443\n",
      "Epoch 9 | val MSE: 0.0434\n",
      "Epoch 10 | val MSE: 0.0427\n",
      "Epoch 11 | val MSE: 0.0419\n",
      "Epoch 12 | val MSE: 0.0422\n",
      "Epoch 13 | val MSE: 0.0416\n",
      "Epoch 14 | val MSE: 0.0413\n",
      "Epoch 15 | val MSE: 0.0404\n",
      "Epoch 16 | val MSE: 0.0403\n",
      "Epoch 17 | val MSE: 0.0400\n",
      "Epoch 18 | val MSE: 0.0398\n",
      "Epoch 19 | val MSE: 0.0394\n",
      "Epoch 20 | val MSE: 0.0394\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and simple training loop\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Enable cuDNN autotune and TF32 on Ampere/ADA\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BabyNetMHCpan().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')\n",
    "\n",
    "\n",
    "def to_one_hot_on_gpu(pep_idx, pseudo_idx):\n",
    "    # pep_idx: (B, Lmax) long with PAD_INDEX padding\n",
    "    pep_oh = F.one_hot(pep_idx, num_classes=PAD_INDEX + 1)[..., :20]  # (B, Lmax, 20)\n",
    "    pep_oh = pep_oh.permute(0, 2, 1).float()  # (B, 20, Lmax)\n",
    "    pseudo_oh = F.one_hot(pseudo_idx, num_classes=20).float()  # (B, 34, 20)\n",
    "    return pep_oh, pseudo_oh\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for pep_idx, pseudo_idx, y in loader:\n",
    "            pep_idx = pep_idx.to(device, non_blocking=True)\n",
    "            pseudo_idx = pseudo_idx.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            pep_oh, pseudo_oh = to_one_hot_on_gpu(pep_idx, pseudo_idx)\n",
    "            with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "                y_pred = model(pep_oh, pseudo_oh)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "            n += y.size(0)\n",
    "    return loss_sum / max(n, 1)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    for pep_idx, pseudo_idx, y in train_loader:\n",
    "        pep_idx = pep_idx.to(device, non_blocking=True)\n",
    "        pseudo_idx = pseudo_idx.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        pep_oh, pseudo_oh = to_one_hot_on_gpu(pep_idx, pseudo_idx)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "            y_pred = model(pep_oh, pseudo_oh)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch} | val MSE: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "369e435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1]) [0.04793084 0.03500205 0.27789724 0.07731942 0.9229736 ]\n"
     ]
    }
   ],
   "source": [
    "# Sanity-check one forward pass\n",
    "pep_idx, pseudo_idx, y = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    pep_oh, pseudo_oh = to_one_hot_on_gpu(pep_idx.to(device, non_blocking=True),\n",
    "                                          pseudo_idx.to(device, non_blocking=True))\n",
    "    out = model(pep_oh, pseudo_oh)\n",
    "print(out.shape, out[:5].squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d06675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.715 | Spearman: 0.656\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics (Pearson/Spearman) on the val set (SciPy optional)\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from scipy.stats import pearsonr, spearmanr  # type: ignore\n",
    "    use_scipy = True\n",
    "except Exception:\n",
    "    use_scipy = False\n",
    "\n",
    "model.eval()\n",
    "all_y, all_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for pep_idx, pseudo_idx, y in val_loader:\n",
    "        y_np = y.numpy().ravel()\n",
    "        pep_idx = pep_idx.to(device, non_blocking=True)\n",
    "        pseudo_idx = pseudo_idx.to(device, non_blocking=True)\n",
    "        pep_oh, pseudo_oh = to_one_hot_on_gpu(pep_idx, pseudo_idx)\n",
    "        pred_np = model(pep_oh, pseudo_oh).cpu().numpy().ravel()\n",
    "        all_y.append(y_np)\n",
    "        all_pred.append(pred_np)\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "\n",
    "if use_scipy:\n",
    "    p = pearsonr(all_y, all_pred)[0]\n",
    "    s = spearmanr(all_y, all_pred)[0]\n",
    "else:\n",
    "    # Pearson without SciPy\n",
    "    y_mean = all_y.mean(); p_mean = all_pred.mean()\n",
    "    num = ((all_y - y_mean) * (all_pred - p_mean)).sum()\n",
    "    den = np.sqrt(((all_y - y_mean) ** 2).sum()) * np.sqrt(((all_pred - p_mean) ** 2).sum())\n",
    "    p = num / (den + 1e-12)\n",
    "    # Spearman via rank correlation\n",
    "    y_rank = all_y.argsort().argsort().astype(float)\n",
    "    p_rank = all_pred.argsort().argsort().astype(float)\n",
    "    yrm = y_rank.mean(); prm = p_rank.mean()\n",
    "    s_num = ((y_rank - yrm) * (p_rank - prm)).sum()\n",
    "    s_den = np.sqrt(((y_rank - yrm) ** 2).sum()) * np.sqrt(((p_rank - prm) ** 2).sum())\n",
    "    s = s_num / (s_den + 1e-12)\n",
    "\n",
    "print(f\"Pearson: {p:.3f} | Spearman: {s:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097e93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model helper and quick inference function\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_path = Path('tmp/baby_netmhcpan.pt')\n",
    "ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_model(model, path=ckpt_path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f'Saved to {path}')\n",
    "\n",
    "\n",
    "def predict_peptide_allele(peptide: str, allele: str):\n",
    "    akey = canonicalize_allele(allele)\n",
    "    if akey not in canon_to_pseudo_34:\n",
    "        raise ValueError(f'No pseudo for allele {allele}')\n",
    "    pep_idx = seq_to_index_tensor(peptide).unsqueeze(0).to(device)\n",
    "    pseudo_idx = seq_to_index_tensor(canon_to_pseudo_34[akey]).unsqueeze(0).to(device)\n",
    "    pep_oh, pseudo_oh = to_one_hot_on_gpu(pep_idx, pseudo_idx)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "        pred = model(pep_oh, pseudo_oh).item()\n",
    "    return pred\n",
    "\n",
    "# Example usage (after training):\n",
    "# save_model(model)\n",
    "# predict_peptide_allele('SIINFEKL', 'H-2-Kb')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
